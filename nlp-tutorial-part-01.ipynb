{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Meets Bags of Popcorn\n",
    "- https://www.kaggle.com/c/word2vec-nlp-tutorial\n",
    "- 튜토리얼을 제공하는 케글 경진대회 중의 하나\n",
    "\n",
    "#### [자연 언어 처리 - 위키백과, 우리 모두의 백과사전](https://ko.wikipedia.org/wiki/%EC%9E%90%EC%97%B0_%EC%96%B8%EC%96%B4_%EC%B2%98%EB%A6%AC)\n",
    "- 자연 언어 처리(自然言語處理) 또는 자연어 처리(自然語處理)는 인간이 발화하는 언어 현상을 기계적으로 분석해서 컴퓨터가 이해할 수 있는 형태로 만드는 자연 언어 이해 혹은 그러한 형태를 다시 인간이 이해할 수 있는 언어로 표현하는 제반 기술을 의미한다. (출처 : 위키피디아)\n",
    "\n",
    "#### 자연어처리(NLP)와 관련된 캐글 경진대회\n",
    "- [Sentiment Analysis on Movie Reviews | Kaggle](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews)\n",
    "- [Toxic Comment Classification Challenge | Kaggle](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)\n",
    "- [Spooky Author Identification | Kaggle](https://www.kaggle.com/c/spooky-author-identification)\n",
    "\n",
    "## 튜토리얼 개요\n",
    "\n",
    "### 파트1\n",
    "- 입문자를 대상으로 기본 자연어 처리(Bag Of Words)를 다룬다.\n",
    "\n",
    "### 파트2, 3\n",
    "- Word2Vec을 사용하여(Word Vectors) 모델을 학습시키는 방법과 감정분석에 단어 벡터를 사용하는 방법을 본다.\n",
    "- 파트3는 레시피를 제공하지 않고 Word2Vec을 사용하는 몇 가지 방법을 실험해 본다.\n",
    "- 파트3에서는 K-means 알고리즘을 사용해 군집화를 해본다.\n",
    "- 긍정과 부정 리뷰가 섞여있는 100,000만개의 IMDB 감정분석 데이터 세트를 통해 목표를 달성해 본다.\n",
    "\n",
    "### 평가 - ROC 커브(Receiver-Operating Characteristic curve)\n",
    "- **TPR**(True Positive Rate)과 **FPR**(False Positive Rate)을 각각 x, y 축으로 놓은 그래프\n",
    "- **민감도 TPR**\n",
    "    - **1인 케이스에 대해 1로 예측한 비율**\n",
    "    - 암환자를 진찰해서 암이라고 진단함\n",
    "- **특이도 FPR**\n",
    "    - **0인 케이스에 대해 1로 잘못 예측한 비율**\n",
    "    - 암환자가 아닌데 암이라고 진단함\n",
    "- X, Y가 둘 다 [0, 1] 범위이고 (0, 0)에서 (1, 1)을 잇는 곡선이다.\n",
    "<img src=\"img/01_roc.png\" width=\"350\">(출처 : 위키피디아)\n",
    "- **ROC 커브의 및 면적이 1에 가까울 수록(왼쪽 꼭지점에 다가갈수록) 좋은 성능(정확도가 높은 평가 방법이다)**\n",
    "- 참고 :\n",
    "    - [New Sight :: Roc curve, AUR(AUCOC), 민감도, 특이도](http://newsight.tistory.com/53)\n",
    "    - [ROC의 AUC 구하기 :: 진화하자 - 어디에도 소속되지 않기](http://adnoctum.tistory.com/121)\n",
    "    - [Receiver operating characteristic - Wikipedia](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n",
    "    \n",
    "### Use Google's Word2Vec for movie reviews\n",
    "- 자연어 텍스트를 분석해서 특정단어를 얼마나 사용했는지, 얼마나 자주 사용했는지, 어떤 종류의 텍스트인지 분류하거나 긍정인지 부정인지에 대한 감정분석, 그리고 어떤 내용인지 요약하는 정보를 얻을 수 있다.\n",
    "- 감정분석은 머신러닝(기계학습)에서 어려운 주제로 풍자, 애매모호한 말, 반어법, 언어 유희로 표현을 하는데 이는 사람과 컴퓨터에게 모두 오해의 소지가 있다. 여기에서는 Word2Vec을 통한 감정분석을 해보는 튜토리얼을 해본다.\n",
    "- Google의 Word2Vec은 단어의 의미와 관계를 이해하는 데 도움\n",
    "- 상당수의 NLP기능은 nltk모듈에 구현되어 있는데 이 모듈은 코퍼스, 함수와 알고리즘으로 구성되어 있다.\n",
    "- 단어 임베딩 모형 테스트 : [Korean Word2Vec](http://w.elnn.kr/search/)\n",
    "\n",
    "### BOW(bag of words)\n",
    "- 가장 간단하지만 효과적이라 널리쓰이는 방법\n",
    "- 장, 문단, 문장, 서식과 같은 입력 텍스트의 구조를 제외하고 각 단어가 이 말뭉치에 얼마나 많이 나타나는지만 헤아린다.\n",
    "- 구조와 상관없이 단어의 출현횟수만 세기 때문에 텍스트를 담는 가방(bag)으로 생각할 수 있다.\n",
    "- BOW는 단어의 순서가 완전히 무시 된다는 단점이 있다. 예를 들어 의미가 완전히 반대인 두 문장이 있다고 하다.\n",
    "    - `it's bad, not good at all.`\n",
    "    - `it's good, not bad at all.`\n",
    "- 위 두 문장은 의미가 전혀 반대지만 완전히 동일하게 반환된다.\n",
    "- **이를 보완하기 위해 n-gram을 사용**하는 데 BOW는 하나의 토큰을 사용하지만 n-gram은 n개의 토큰을 사용할 수 있도록 한다.\n",
    "\n",
    "* [Bag-of-words model - Wikipedia](https://en.wikipedia.org/wiki/Bag-of-words_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 파트 1\n",
    "\n",
    "- **NLP는?**<br>\n",
    "NLP(자연어처리)는 텍스트 문제에 접근하기 위한 기술집합이다.\n",
    "<br>\n",
    "<br>\n",
    "- 이 튜토리얼에서는 IMDB 영화 리뷰를 로딩하고 정제하고 간단한 BOW(Bag of Words) 모델을 적용하여 리뷰가 추천인지 아닌지에 대한 정확도를 예측한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"At eight o'clock on Thursday morning Arthur didn't feel very good.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At',\n",
       " 'eight',\n",
       " \"o'clock\",\n",
       " 'on',\n",
       " 'Thursday',\n",
       " 'morning',\n",
       " 'Arthur',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'feel',\n",
       " 'very',\n",
       " 'good',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('At', 'IN'),\n",
       " ('eight', 'CD'),\n",
       " (\"o'clock\", 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('Thursday', 'NNP'),\n",
       " ('morning', 'NN'),\n",
       " ('Arthur', 'NNP'),\n",
       " ('did', 'VBD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('feel', 'VB'),\n",
       " ('very', 'RB'),\n",
       " ('good', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# %ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`header = 0` 은 파일의 첫 번째 줄에 열 이름이 있음을 나타내며<br>\n",
    "`delimiter = \\t` 는 필드가 탭으로 구분되는 것을 의미한다.<br>\n",
    "`quoting = 3` 은 쌍따옴표를 무시하도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('data/labeledTrainData.tsv',\n",
    "                   header=0, delimiter='\\t', quoting=3)\n",
    "train.shape\n",
    "\n",
    "test = pd.read_csv('data/testData.tsv',\n",
    "                   header=0, delimiter='\\t', quoting=3)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>\"3453_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It seems like more consideration has gone int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>\"5064_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I don't believe they made this film. Complete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>\"10905_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Guy is a loser. Can't get girls, needs to bui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>\"10194_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"This 30 minute documentary Buñuel made in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>\"8478_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I saw this movie as a child and it broke my h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  sentiment                                             review\n",
       "24995   \"3453_3\"          0  \"It seems like more consideration has gone int...\n",
       "24996   \"5064_1\"          0  \"I don't believe they made this film. Complete...\n",
       "24997  \"10905_3\"          0  \"Guy is a loser. Can't get girls, needs to bui...\n",
       "24998  \"10194_3\"          0  \"This 30 minute documentary Buñuel made in the...\n",
       "24999   \"8478_8\"          1  \"I saw this movie as a child and it broke my h..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>\"2155_10\"</td>\n",
       "      <td>\"Sony Pictures Classics, I'm looking at you! S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>\"59_10\"</td>\n",
       "      <td>\"I always felt that Ms. Merkerson had never go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>\"2531_1\"</td>\n",
       "      <td>\"I was so disappointed in this movie. I am ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>\"7772_8\"</td>\n",
       "      <td>\"From the opening sequence, filled with black ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>\"11465_10\"</td>\n",
       "      <td>\"This is a great horror film for people who do...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                             review\n",
       "24995   \"2155_10\"  \"Sony Pictures Classics, I'm looking at you! S...\n",
       "24996     \"59_10\"  \"I always felt that Ms. Merkerson had never go...\n",
       "24997    \"2531_1\"  \"I was so disappointed in this movie. I am ver...\n",
       "24998    \"7772_8\"  \"From the opening sequence, filled with black ...\n",
       "24999  \"11465_10\"  \"This is a great horror film for people who do..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'sentiment', 'review'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 튜토리얼01 에서는, <br>\n",
    "test dataset에 'sentiment'레이블이 없다. <br>\n",
    "train data의 'sentiment'를 바탕으로 test 데이터를 기계학습을 통해 예측한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'review'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         25000 non-null  object\n",
      " 1   sentiment  25000 non-null  int64 \n",
      " 2   review     25000 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 586.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.50001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment\n",
       "count  25000.00000\n",
       "mean       0.50000\n",
       "std        0.50001\n",
       "min        0.00000\n",
       "25%        0.00000\n",
       "50%        0.50000\n",
       "75%        1.00000\n",
       "max        1.00000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12500\n",
       "0    12500\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"With all this stuff going down at the moment with MJ i\\'ve started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ\\'s feeling towards the press and also the obvious message of drugs are bad m\\'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely lik'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫번째 데이터 불러와서 보기 \n",
    "\n",
    "train['review'][0][:700]  # [0]첫번째 모두 불러오면 너무 많아서 -> [:700]자 까지만 불러오기 ***\n",
    "# <br> 등의 html 태그가 섞여있기 때문에 => 이를 정제해줄 필요가 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기까지 <br>\n",
    "- pandas 로 데이터 불러오기\n",
    "- 데이터 정제하기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 정제 Data Cleaning and Text Preprocessing(전처리 과정)\n",
    "- 데이터 정제가 필요한 이유 : <br>\n",
    "기계가 텍스트를 이해할 수 있도록 텍스트를 정제해 준다.<br>\n",
    "신호와 소음을 구분한다. 아웃라이어데이터로 인한 오버피팅을 방지한다.<br>\n",
    "<br>\n",
    "- 정제할 4가지 과정 :\n",
    "    1. BeautifulSoup(뷰티풀숩)을 통해 HTML 태그를 제거\n",
    "    2. 정규표현식으로 알파벳 이외의 문자를 공백으로 치환\n",
    "    3. NLTK 데이터를 사용해 불용어(Stopword)를 제거\n",
    "    4. 어간추출(스테밍 Stemming)과 음소표기법(Lemmatizing)의 개념을 이해하고 SnowballStemmer를 통해 어간을 추출\n",
    "    (불용어 : ex) I, me, my 같이 자주 등장하지만 문장의 의미가 많지 않은 단어들)\n",
    "\n",
    "\n",
    "### 텍스트 데이터 전처리 이해하기\n",
    "- 한국어 데이터 텍스트 전처리의 경우, KoNLPy 를 사용한다.\n",
    "- KoNLPy 는 트위터 형태소 분석기를 통해 만들어졌다. \n",
    "(출처 : [트위터 한국어 형태소 분석기](https://github.com/twitter/twitter-korean-text))\n",
    "\n",
    "**정규화 normalization (입니닼ㅋㅋ -> 입니다 ㅋㅋ, 샤릉해 -> 사랑해)**\n",
    "\n",
    "* 한국어를 처리하는 예시입니닼ㅋㅋㅋㅋㅋ -> 한국어를 처리하는 예시입니다 ㅋㅋ\n",
    "\n",
    "**토큰화 tokenization**\n",
    "\n",
    "* 한국어를 처리하는 예시입니다 ㅋㅋ -> 한국어Noun, 를Josa, 처리Noun, 하는Verb, 예시Noun, 입Adjective, 니다Eomi ㅋㅋKoreanParticle\n",
    "\n",
    "**어근화 stemming (입니다 -> 이다)**\n",
    "\n",
    "* 한국어를 처리하는 예시입니다 ㅋㅋ -> 한국어Noun, 를Josa, 처리Noun, 하다Verb, 예시Noun, 이다Adjective, ㅋㅋKoreanParticle\n",
    "\n",
    "**어구 추출 phrase extraction** \n",
    "\n",
    "* 한국어를 처리하는 예시입니다 ㅋㅋ -> 한국어, 처리, 예시, 처리하는 예시\n",
    "\n",
    "Introductory Presentation: [Google Slides](https://docs.google.com/presentation/d/10CZj8ry03oCk_Jqw879HFELzOLjJZ0EOi4KJbtRSIeU/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 뷰티풀숩이 설치되지 않았다면 우선 설치해 준다.<br>\n",
    "`pip install BeautifulSoup4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: beautifulsoup4\n",
      "Version: 4.6.0\n",
      "Summary: Screen-scraping library\n",
      "Home-page: http://www.crummy.com/software/BeautifulSoup/bs4/\n",
      "Author: Leonard Richardson\n",
      "Author-email: leonardr@segfault.org\n",
      "License: MIT\n",
      "Location: c:\\users\\soomin\\anaconda3\\lib\\site-packages\n",
      "Requires: \n",
      "Required-by: konlpy, conda-build\n"
     ]
    }
   ],
   "source": [
    "# 설치 및 버전확인\n",
    "!pip show BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely lik\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"With all this stuff going down at the moment with MJ i\\'ve started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ\\'s feeling towards the press and also the obvious message of drugs are bad m\\'kay.Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyw'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BeautifulSoup 불러와서, example1 에 담아주기 \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "example1 = BeautifulSoup(train['review'][0], \"html5lib\")\n",
    "\n",
    "# BeautifulSoup 으로 정제 전과 비교하기 \n",
    "print(train['review'][0][:700])\n",
    "\n",
    "# html 테그 등이 제거됨을 볼 수 있다\n",
    "example1.get_text()[:700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' With all this stuff going down at the moment with MJ i ve started listening to his music  watching the odd documentary here and there  watched The Wiz and watched Moonwalker again  Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent  Moonwalker is part biography  part feature film which i remember going to see at the cinema when it was originally released  Some of it has subtle messages about MJ s feeling towards the press and also the obvious message of drugs are bad m kay Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyw'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫번째 데이터를, \n",
    "# 정규표현식을 사용해서 특수문자를 제거하기 \n",
    "\n",
    "import re\n",
    "# 소문자와 대문자가 아닌 것은 공백으로 대체한다.\n",
    "letters_only = re.sub('[^a-zA-Z]', ' ', example1.get_text())  \n",
    "# ^ : Not 의미 \n",
    "# a 부터 Z 의 문자가 아니라면, ' ' 공백으로 대체한다 \n",
    "\n",
    "letters_only[:700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['with',\n",
       " 'all',\n",
       " 'this',\n",
       " 'stuff',\n",
       " 'going',\n",
       " 'down',\n",
       " 'at',\n",
       " 'the',\n",
       " 'moment',\n",
       " 'with']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모두 소문자로 변환한다.\n",
    "lower_case = letters_only.lower()\n",
    "\n",
    "# 문자를 나눈다. => 토큰화 : split 을 사용해서 소문자로 변경된 단어들을 토큰화 하기 \n",
    "words = lower_case.split()\n",
    "print(len(words))\n",
    "words[:10]\n",
    "# 437개의 토큰으로 이루어짐 \n",
    "# 10개만 보기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불용어 제거(Stopword Removal)\n",
    "\n",
    "일반적으로 코퍼스에서 자주 나타나는 단어는 학습 모델로서 학습이나 예측 프로세스에 실제로 기여하지 않아 다른 텍스트와 구별하지 못한다. 예를들어 조사, 접미사, i, me, my, it, this, that, is, are 등 과 같은 단어는 **빈번하게 등장하지만 실제 의미를 찾는데 큰 기여를 하지 않는다. Stopwords는 \"to\"또는 \"the\"와 같은 용어를 포함하므로 사전 처리 단계에서 제거하는 것이 좋다.** NLTK에는 153 개의 영어 불용어가 미리 정의되어 있다. 17개의 언어에 대해 정의되어 있으며 한국어는 없다.\n",
    "\n",
    "\n",
    "### NLTK data 설치 \n",
    "* http://corazzon.github.io/nltk_data_install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['stuff',\n",
       " 'going',\n",
       " 'moment',\n",
       " 'mj',\n",
       " 'started',\n",
       " 'listening',\n",
       " 'music',\n",
       " 'watching',\n",
       " 'odd',\n",
       " 'documentary']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stopwords 를 제거한 뒤의 변화된 토큰의 개수  437 -> 219 개가 됨 \n",
    "words = [w for w in words if not w in stopwords.words('english')]\n",
    "print(len(words))\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스테밍(어간추출, 형태소 분석)\n",
    "출처 : [어간 추출 - 위키백과, 우리 모두의 백과사전](https://ko.wikipedia.org/wiki/%EC%96%B4%EA%B0%84_%EC%B6%94%EC%B6%9C)\n",
    "\n",
    "\n",
    "* 어간 추출(語幹 抽出, 영어: stemming)은 어형이 변형된 단어로부터 접사 등을 제거하고 그 단어의 어간을 분리해 내는 것\n",
    "* \"message\", \"messages\", \"messaging\" 과 같이 복수형, 진행형 등의 문자를 같은 의미의 단어로 다룰 수 있도록 도와준다.\n",
    "* stemming(형태소 분석): 여기에서는 NLTK에서 제공하는 형태소 분석기를 사용한다. \n",
    "    - **포터 형태소 분석기는 보수적이고, 랭커스터 형태소 분석기는 좀 더 적극적이다.** \n",
    "    - 형태소 분석 규칙의 적극성 때문에 랭커스터 형태소 분석기는 더 많은 동음이의어 형태소를 생산한다.\n",
    "    - [참고 : 모두의 데이터 과학 with 파이썬(길벗)](http://www.gilbut.co.kr/book/bookView.aspx?bookcode=BN001787)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 포터 스태머의 사용 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum\n",
      "The stemmed form of running is: run\n",
      "The stemmed form of runs is: run\n",
      "The stemmed form of run is: run\n"
     ]
    }
   ],
   "source": [
    "# 포터 스태머\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "print(stemmer.stem('maximum'))\n",
    "print(\"The stemmed form of running is: {}\".format(stemmer.stem(\"running\")))\n",
    "print(\"The stemmed form of runs is: {}\".format(stemmer.stem(\"runs\")))\n",
    "print(\"The stemmed form of run is: {}\".format(stemmer.stem(\"run\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 랭커스터 스태머의 사용 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxim\n",
      "The stemmed form of running is: run\n",
      "The stemmed form of runs is: run\n",
      "The stemmed form of run is: run\n"
     ]
    }
   ],
   "source": [
    "# 랭커스터 스태머\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "print(lancaster_stemmer.stem('maximum'))\n",
    "print(\"The stemmed form of running is: {}\".format(lancaster_stemmer.stem(\"running\")))\n",
    "print(\"The stemmed form of runs is: {}\".format(lancaster_stemmer.stem(\"runs\")))\n",
    "print(\"The stemmed form of run is: {}\".format(lancaster_stemmer.stem(\"run\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> running, runs, run 의 경우에는, <br>\n",
    "포터스태머 와 랭커스태머의 차이가 없음을 알 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
